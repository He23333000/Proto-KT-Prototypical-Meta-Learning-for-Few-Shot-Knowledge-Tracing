\documentclass[tikz,border=10pt]{standalone}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit,calc,backgrounds}
\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}

\definecolor{embedblue}{RGB}{100,149,237}
\definecolor{transgreen}{RGB}{60,179,113}
\definecolor{predorange}{RGB}{255,140,0}

\tikzstyle{layer} = [rectangle, draw, fill=blue!20, text width=4cm, text centered, rounded corners, minimum height=1cm, thick]
\tikzstyle{embed} = [rectangle, draw, fill=embedblue!20, text width=3cm, text centered, rounded corners, minimum height=0.8cm]
\tikzstyle{trans} = [rectangle, draw, fill=transgreen!20, text width=4cm, text centered, rounded corners, minimum height=1.2cm, thick]
\tikzstyle{pred} = [rectangle, draw, fill=predorange!20, text width=3cm, text centered, rounded corners, minimum height=0.8cm]
\tikzstyle{arrow} = [thick,->,>=stealth]

\begin{tikzpicture}[node distance=1cm, auto]

% Title
\node[text width=10cm, text centered, font=\Large\bfseries] (title) at (0,11) {SAKT Architecture (Self-Attentive Knowledge Tracing)};

% Input Layer
\node[embed, text width=5cm] (input) at (0,9.5) {
    \textbf{Input Sequences}\\
    Questions: $\{q_1, q_2, \ldots, q_n\}$\\
    Responses: $\{r_1, r_2, \ldots, r_n\}$
};

% Embedding Layer
\node[font=\large\bfseries] (embedtitle) at (-4,8) {Embedding Layer};

\node[embed] (qembed) at (-3,6.5) {Question\\Embedding\\$\mathbf{E}_q \in \mathbb{R}^{d}$};
\node[embed] (rembed) at (0,6.5) {Response\\Embedding\\$\mathbf{E}_r \in \mathbb{R}^{d}$};
\node[embed] (sembed) at (3,6.5) {Skill\\Embedding\\$\mathbf{E}_s \in \mathbb{R}^{d}$};

\draw[arrow] (input) -- ++(-3,-1) -- (qembed);
\draw[arrow] (input) -- ++(0,-1) -- (rembed);
\draw[arrow] (input) -- ++(3,-1) -- (sembed);

% Positional Encoding
\node[embed, text width=5cm] (posembed) at (0,5) {
    Positional Encoding (Relative)\\
    $\mathbf{PE}(i,j) = \text{bias}(\text{bucket}(i-j))$
};

\draw[arrow] (qembed) -- (posembed);
\draw[arrow] (rembed) -- (posembed);
\draw[arrow] (sembed) -- (posembed);

% Combined Embedding
\node[embed, text width=5cm] (combined) at (0,3.5) {
    Combined Embedding\\
    $\mathbf{x}_i = \mathbf{E}_q(q_i) + \mathbf{E}_r(r_i) + \mathbf{E}_s(s_i) + \mathbf{PE}_i$
};

\draw[arrow] (posembed) -- (combined);

% Transformer Blocks
\node[font=\large\bfseries] (transtitle) at (-4,2) {Transformer Blocks ($\times L$)};

% Block 1
\node[trans, text width=5cm, minimum height=2cm] (trans1) at (0,1) {
    \textbf{Multi-Head Self-Attention}\\
    $Q = \mathbf{x}W_Q, K = \mathbf{x}W_K, V = \mathbf{x}W_V$\\
    $\text{Attn}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + \text{bias}\right)V$\\
    + Layer Norm + Residual
};

\draw[arrow] (combined) -- (trans1);

% Block 2
\node[trans, text width=5cm, minimum height=1.5cm] (trans2) at (0,-1.5) {
    \textbf{Feed-Forward Network}\\
    $\text{FFN}(\mathbf{h}) = \text{ReLU}(\mathbf{h}W_1 + b_1)W_2 + b_2$\\
    + Layer Norm + Residual
};

\draw[arrow] (trans1) -- (trans2);

% More blocks indicator
\node[font=\large] (dots) at (0,-3) {$\vdots$ (repeat $L$ times)};

\draw[arrow] (trans2) -- (dots);

% Final hidden states
\node[trans, text width=5cm] (hidden) at (0,-4) {
    Final Hidden States\\
    $\mathbf{h} = \{\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_n\} \in \mathbb{R}^{n \times d}$
};

\draw[arrow] (dots) -- (hidden);

% Prediction Head
\node[font=\large\bfseries] (predtitle) at (-4,-5.5) {Prediction Head};

\node[pred, text width=5cm] (predmean) at (0,-6.5) {
    Mean Prediction\\
    $\hat{y}_i = \sigma(\mathbf{h}_i^\top W_{\text{pred}} + b)$
};

\node[pred, text width=5cm] (predvar) at (0,-7.8) {
    Uncertainty (Optional)\\
    $\log \sigma^2_i = \mathbf{h}_i^\top W_{\text{var}} + b_{\text{var}}$
};

\draw[arrow] (hidden) -- (predmean);
\draw[arrow] (hidden) -- (predvar);

% Output
\node[pred, text width=5cm] (output) at (0,-9.5) {
    \textbf{Output}\\
    Correctness Probability: $\hat{y}_i \in [0, 1]$\\
    Uncertainty: $\sigma^2_i$ (optional)
};

\draw[arrow] (predmean) -- (output);
\draw[arrow] (predvar) -- (output);

% Side annotations
\node[draw, thick, rounded corners, fill=yellow!10, text width=4.5cm, align=left] (notes) at (7,3) {
    \textbf{Key Features:}\\[0.2cm]
    
    \textbullet\ Causal masking for autoregressive prediction\\[0.2cm]
    
    \textbullet\ Relative position encodings (T5-style)\\[0.2cm]
    
    \textbullet\ Additive embeddings for flexibility\\[0.2cm]
    
    \textbullet\ Multi-head attention captures dependencies\\[0.2cm]
    
    \textbullet\ Optional uncertainty estimation
};

% Dimensions box
\node[draw, thick, rounded corners, fill=cyan!10, text width=4.5cm, align=left, below=0.5cm of notes] (dims) {
    \textbf{Typical Dimensions:}\\[0.2cm]
    
    $d$: Embedding dim (128-256)\\[0.2cm]
    $L$: Num layers (2-4)\\[0.2cm]
    $h$: Num heads (4-8)\\[0.2cm]
    $d_{\text{ff}}$: FFN dim (512-1024)\\[0.2cm]
    $n$: Sequence length (variable)
};

\end{tikzpicture}

\end{document}

